\section{Related Works}
\subsection{Defense Mechanisms for Vision Language Models}
In the ongoing effort to strengthen VLMs against jailbreak threats, researchers have proposed various defensive strategies, which can be broadly classified into three main categories: 1) 

In the continuous quest to fortify VLMs against jailbreak threats, researchers have proposed various strategies. They can be broadly categorized into three main approaches: 1) (\textit{Model Fine-tuning-based defences}) These defences involve fine-tuning the VLM to enhance safety techniques, including leveraging natural language feedback for improved alignment~\citep{chen2024dressinstructinglargevisionlanguage} and adversarial training to increase model robustness. Parameter adjustments to resist adversarial prompts and images are also employed~\citep{wang2024adashield}. 2) (\textit{Response Evaluation-based defences}) These approaches assess the harmfulness of VLM responses, often followed by iterative refinement to ensure safe outputs. Methods integrate harm detection and detoxification to correct potentially harmful outputs~\citep{pi2024mllm}. the newly devised strategy "\textit{E}yes \textit{C}losed, \textit{S}afety \textit{O}n" (ECSO)~\citep{gou2024eyes} restores the intrinsic safety mechanism of pre-aligned LLMs by transforming potentially malicious visual content into plain text. 3) (\textit{Prompt Perturbation-based defences}) These strategies involve altering input prompts to neutralize adversarial effects. Techniques use variant generators to disturb input queries and analyze response consistency to identify potential jailbreak attempts~\citep{zhang2023mutation}.
\subsection{Randomized Smoothing}
\label{rs}
Randomized smoothing is a versatile certification technique applicable to any model with black-box access. Initially proposed as a heuristic defense~\citep{cao2017mitigating,liu2018towards}, \cite{lecuyer2019certifiedrobustnessadversarialexamples} later provided robustness guarantees using differential privacy. \cite{cohen2019certifiedadversarialrobustnessrandomized} demonstrated that if a base classifier $f(\mathbf{x})$ is empirically robust under Gaussian noise $\mathbf{e}\sim\mathcal{N}(\mathbf{0}, \sigma^2\mathbf{I})$, the smoothed classifier $g(\mathbf{x})=\mathop{\arg\max}_c\mathbb{P}(f(\mathbf{x}+\mathbf{e})=c)$ is certifiably robust against $\ell_2$ norm-based adversaries, with robustness radius 
$\delta=\frac{\sigma}{2}(\Phi^{-1}(\underline{p_A})-\Phi^{-1}(\overline{p_B}))$, where $\underline{p_A}$ and $\overline{p_B}$ are the lower bound probability of major class, and upper bound probability of runner-up class, respectively. Later, \cite{salman2020denoised} extended this to pre-trained models. However, studies~\citep{yang2020randomizedsmoothingshapessizes,kumar2020curse} showed that for $\ell_p$ norm-based attacks (p$>$2), the certification radius shrinks as $\mathcal{O}(1/d^{\frac{1}{2}-\frac{1}{p}})$, approaching zero for high-dimensional data. To address other norm-based attacks, smoothing with different distributions--such as Uniform, Laplacian, and non-Gaussian--has been explored for $\ell_0$~\citep{lee2019tight}, $\ell_1$~\citep{teng2020ell_1}, and $\ell_\infty$~\citep{zhang2020filling} norm-based attacks, respectively. 