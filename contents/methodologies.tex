\section{Methodologies}
This section will first introduce fundamental preliminaries regarding the distributional security range in vision language models. Then, we will illustrate in detail how to apply randomized smoothing to certify the security range of the prompt tuple in vision language models. 

\subsection{Preliminaries}
Denote the unperturbed visual prompt and the harmful textual prompt using $x_c$ and $t_h$ respectively, then $p_{\theta}^{\tau}(r | \left<x_c, t_h\right>)$ represents a bi-modal prompt-grounded vision language model parameterized by $\theta$, where $r$ indicates the textual response given $\left<x_c, t_h\right>$, $\tau$ indicates the temperature used in the inference stage, the lower $\tau$ is, the more deterministic $p^{\tau}_{\theta}$ is. 
\subsection{Targeted Distance in Vision Language Model Reasoning}
Previous research has focused on manipulating visual prompts to create jailbreak adversaries on $p_{\theta}^{\tau}$. This includes additive and semantic modifications in the input space, which cannot be theoretically bounded using the $\ell_p$ norm. Therefore, we focus on the visual feature $\mathcal{E}_v(x_c)$ instead of $x_c$, and simplify the textual response to $r_{\theta, \tau}^{\left<\mathcal{E}_v(x_c), t_h\right>}$, where $\mathcal{E}_v$ refers to the CLIP-based visual feature extractor. Consequently, determining whether the textual response is toxic is critical. We propose a novel distance metric to measure the difference between the generated textual response and the targeted toxic one to address this.
\begin{definition}
[$\mathbf{r}_{t_h}^{M}$-Targeted Distance]
\label{tsr}
Given the harmful response $r_{t_h}^{0}$, if we denote the set of semantically similar harmful responses regarding $r_{t_h}^0$ as $\mathbf{r}_{t_h}^{M} = \{r_{t_h}^0, r_{t_h}^1, \cdots, r_{t_h}^M\}$$(M\geq0)$, the distance function between responses as $\mathcal{D}(\cdot, \cdot)$, and an infinitesimal constant as $\tau_1$, then we can averagely indicate $\mathbb{E}_{r_{t_h}\in\mathbf{r}_{t_h}^M}[\mathcal{D}(r_{\theta, \tau_1}^{\left<\mathcal{E}_v(x_c), t_h\right>}, r_{t_h})]$ as the $\left<\mathbf{r}_{t_h}^M, \tau_1\right>$-targeted distance of $\mathcal{E}_v$-based $p_{\theta}^{\tau_1}$ on $\left<x_c, t_h\right>$.
\end{definition}
Specifically, due to the probabilistic nature of $p_{\theta}^{\tau}$ during the inference stage, the generated response can vary across i.i.d. queries. Consequently, we identify the generated response with the highest probability by setting a small constant $\tau_1$. This detail is not included in the naming of Def.~\ref{tsr} as it remains consistent across different prompt tuples. In practical scenarios, various toxic responses may be semantically synonymous with a specific $t_h$, undermining the validity of quantifying the targeted secure distance using only one instance. Therefore, we calculate the expected distance over a set of responses to ensure a more robust measurement.
% Generally, per Eq.~\ref{tsr-eq}, for each similarity-bounded prompt tuple $\left<x_c^{\prime}, t_h\right>$, it is essential to ensure that the upper bound of the negativity of the distance between its response with the targeted toxic response does not exceed a predefined threshold. 

Def.~\ref{tsr} formulates the secure distance by incorporating a variety of semantically similar responses and in this context, designing an appropriate distance function $\mathcal{D}(\cdot, \cdot)$ that distinctly delineates the difference between responses presents an intriguing challenge. Specifically, the cosine similarity between encoded features of two sentences is extensively used to evaluate their physical distance in contrastive learning~\citep{jaiswal2020survey,9226466,liu2021self}. However, due to the semantic drift, relying solely on cosine similarity is insufficient for accurately measuring the physical distance between two responses. To mitigate this limitation in our cases, we introduce RoBERTa~\citep{vidgen2021}, which additionally evaluates the sentence toxicity and somewhat trades off the single criterion of cosine similarity. Consequently, it provides a more reasonable distance measurement between $r_{\theta, \tau_1}^{\left<\mathcal{E}_v(x_c), t_h\right>}$ and a randomly selected harmful response $r_{t_h}$, which is denoted as follows:
\begin{align}
\begin{split}
&\mathcal{D}(r_{\theta, \tau_1}^{\left<\mathcal{E}_v(x_c), t_h\right>}, r_{t_h}) \\
&= 1 - (\lambda\cdot\Gamma(r_{\theta, \tau_1}^{\left<\mathcal{E}_v(x_c), t_h\right>}) + (1-\lambda)\cdot C(r_{\theta, \tau_1}^{\left<\mathcal{E}_v(x_c), t_h\right>}, r_{t_h}))
\end{split}
\end{align}
Here, $\Gamma(\cdot)\in[0,1]$ indicates the toxicity score of the given sentence, while $C(\cdot, \cdot)\in[0,1]$ denotes the cosine similarity between two responses. The parameter $\lambda$ serves as the trade-off factor. Generally, an increase in both the toxicity score and the cosine similarity reduces the distance between the generated response and the harmful response, conversely, as the toxicity score and the cosine similarity decrease, the distance increases.
\subsection{Probabilistic Certification for Targeted Distance}
Define the similarity metric between encoded visual features as $d(\cdot, \cdot)$,  then for any $\mathcal{E}_v(x_c)^{\prime}$ that satisfies $d(\mathcal{E}_v(x_c), \mathcal{E}_v(x_c)^{\prime})\leq\delta$, where $\delta$ is a predefined threshold for visual features, we want to ensure that    $\mathbb{E}_{r_{t_h}\in\mathbf{r}_{t_h}^{M}}[\mathcal{D}(r_{\theta, \tau_1}^{\left<\mathcal{E}_v(x_c)^{\prime}, t_h\right>}, r_{t_h})] \geq \epsilon$, where $\epsilon$ indicates the tolerance threshold for the $\mathbf{r}_{t_h}^{M}$-targeted distance.
However, due to the infinity of $\mathcal{E}_v(x_c)^{\prime}$, we cannot certify the lower bound of their $\mathbf{r}_{t_h}^{M}$-targeted distances using brute-force methods. To solve this problem, we turn to randomized smoothing as illustrated in Sec.~\ref{rs}, and smooth $\mathcal{E}_{v}(x_c)^{\prime}$ with added Gaussian noise and formulate the probabilistic certificate as follows: \xiaowei{again, you are stating what you did, but not the barriers you have to cross with your technical means -- it only reads like you are opting for an easy fix. I would try to explain the technical hardness you must resolve to reach the two Lemmas.}
\begin{definition}[Randomly Smoothed Probabilistic Certificate] 
\label{prob-cert}
Given the randomness of $\mathbf{n}\sim\mathbb{D}$, $P\in\left[0, 1\right]$,
$\mathcal{E}_v$-based $p_{\theta}^{\tau_1}$ has a probabilistically certifiable $\epsilon$-constrained $\left<\mathbf{r}_{t_h}^{M}, \tau_1\right>$-targeted distance on $\left<x_c, t_h\right>$ if and only if for every $\mathcal{E}_v(x_c)^{\prime}$ satisfying $d(\mathcal{E}_v(x_c), \mathcal{E}_v(x_c)^{\prime})\leq\delta$, we have:
\begin{equation}
\label{eq-prob-cert}
\mathbb{P}(\mathbb{E}_{r_{t_h}\in\mathbf{r}_{t_h}^M}[\mathcal{D}(r_{\theta, \tau_1}^{\left<\mathcal{E}_v(x_c)^{\prime}+\mathbf{n}, t_h\right>}, r_{t_h})]\geq\epsilon)\geq P
\end{equation}
\end{definition}
Def.~\ref{prob-cert} indicates that given the $\delta$-bounded region of visual features around $\mathcal{E}_v(x_c)$, there is a maximum 1-$P$ fraction of generated visual features whose $\mathbf{r}_{t_h}^{M}$-targeted distance is smaller than $\epsilon$. Specifically, $\epsilon$ divides targeted distances into two subspaces. Moreover, we exploit the same randomness in Def.~\ref{prob-cert} and denote the randomly smoothed lower bound of $\mathbb{P}(\mathbb{E}_{r_{t_h}\in\mathbf{r}_{t_h}^M}[\mathcal{D}(r_{\theta, \tau_1}^{\left<\mathcal{E}_v(x_c)+\mathbf{n}, t_h\right>}, r_{t_h})]\geq\epsilon)$ as $\tilde{P}$. 
\begin{lemma}[Adaptively Certified $\ell_2$ Radius]
\label{certified_l2}
If we use the $\ell_2$ norm to define the distance function $d(\cdot, \cdot)$, and model the randomness in Def.~\ref{prob-cert} as Gaussian variables, where $\mathbf{n}\sim\mathcal{N}(\mathbf{0}, \sigma^2\mathbf{I})$, then given a constant $\beta>1$, to ensure that $\mathbb{P}(\mathbb{E}_{r_{t_h}\in\mathbf{r}_{t_h}^M}[\mathcal{D}(r_{\theta, \tau_1}^{\left<\mathcal{E}_v(x_c)^{\prime}+\mathbf{n}, t_h\right>}, r_{t_h})]\geq\epsilon)\geq\mathcal{T}$, we could adaptively adjust the range of $\delta$ following the piece-wise inequalities below:
\begin{itemize}
\scriptsize
\item$\delta<\min(\sigma\Phi^{-1}(\tilde{P}),\sigma(\Phi^{-1}(\tilde{P})-\Phi^{-1}(1-\frac{1}{\beta}(\frac{2e(\beta-1)}{\pi})^{\frac{1}{2}})))$, \text{if }$\frac{1}{2}<\mathcal{T}\leq1-\frac{1}{2\beta}(\frac{2e(\beta-1)}{\pi})^{\frac{1}{2}}$;
\item$\delta<\min(\sigma(\Phi^{-1}(\tilde{P})-\Phi^{-1}(1-\frac{1}{\beta}(\frac{2e(\beta-1)}{\pi})^{\frac{1}{2}})),\sigma(\Phi^{-1}(\tilde{P})-\sqrt{\frac{-4\ln(2\beta(1-\mathcal{T})(\frac{\pi}{2e(\beta-1)})^{\frac{1}{2}})}{\beta}}))$, \text{if }$1-\frac{1}{2\beta}(\frac{2e(\beta-1)}{\pi})^{\frac{1}{2}}<\mathcal{T}\leq\frac{1}{2}(\Phi(\Phi^{-1}(\tilde{P})-\frac{\delta}{\sigma})+1)$;
\item$\delta >\max(\sigma\Phi^{-1}(\tilde{P}),\sigma(\Phi^{-1}(\tilde{P})+\Phi^{-1}(1-\frac{1}{\beta}(\frac{2e(\beta-1)}{\pi})^{\frac{1}{2}})))$, \text{if }$\frac{1}{2\beta}(\frac{2e(\beta-1)}{\pi})^{\frac{1}{2}}\leq \mathcal{T}<\frac{1}{2}$;
\item$\delta>\max(\sigma(\Phi^{-1}(\tilde{P})+\Phi^{-1}(1-\frac{1}{\beta}(\frac{2e(\beta-1)}{\pi})^{\frac{1}{2}})),\sigma(\Phi^{-1}(\tilde{P})+\sqrt{\frac{-2\ln(2\beta\mathcal{T}(\frac{\pi}{2e(\beta-1)})^{\frac{1}{2}})}{\beta}}))$, \text{if }$\mathcal{T}<\frac{1}{2\beta}(\frac{2e(\beta-1)}{\pi})^{\frac{1}{2}}$
\end{itemize}
\end{lemma}
Detailed proofs of the above Lemma can be found in \ref{proof-lemma-1}. Different from the robustness certification in binary classification tasks, where we can claim $P_1>P_2$ to ensure that the classifying results would be unchangeable if $\delta<\sigma\Phi^{-1}(\tilde{P})$, we provide the range of $\delta$ not only functioned by $\tilde{P}$ but also varied with the desired probability difference between $P_1$ and $P_2$. 
\begin{lemma}[Certified $\ell_1$ Radius for Probabilistic Gap]
\label{certified_l1}
If we exploit $\ell_1$ norm to specify $d(\cdot, \cdot)$, and define the randomness in Def.~\ref{prob-cert} as Laplace variables, where $\mathbf{n}\sim\Lambda(\mathbf{0}, \lambda)$, then if we want to ensure that $\mathbb{P}(\mathbb{E}_{r_{t_h}\in\mathbf{r}_{t_h}^M}[\mathcal{D}(r_{\theta, \tau_1}^{\left<\mathcal{E}_v(x_c)^{\prime}+\mathbf{n}, t_h\right>}, r_{t_h})]\geq\epsilon)\geq\mathcal{T}$, we need to guarantee that $\delta \leq \left\|\mathcal{E}_v(x_c)\right\|_1-\lambda d\ln\frac{(1-\tilde{P})}{1-\mathcal{T}}$.
\end{lemma}
