\section{Introduction}

To counteract various jailbreak attacks, researchers have proposed several strategies to strengthen VLMs against such threats. One prominent approach involves model fine-tuning-based defences~\citep{wang2024adashield,chen2024dressinstructinglargevisionlanguage}, which aim to intercept and mitigate malicious prompts during training utilizing techniques like prompt optimization and natural language feedback to enhance the model's robustness. Another method, known as response evaluation-based defences~\citep{pi2024mllmprotectorensuringmllmssafety,gou2024eyesclosedsafetyon}, operates during the inference phase to ensure the model's responses to potentially harmful prompts remain aligned with desired ethical standards.
Additionally, prompt perturbation-based defences~\citep {zhang2023mutation} take a different approach by transforming input prompts into mutated queries and analyzing the consistency of the model's responses to detect potential jailbreak attempts. This method exploits the inherent fragility of adversarial queries, adding an extra layer of protection against manipulative inputs.

Despite the advancements in defence mechanisms, most approaches remain heuristic and are primarily based on empirical observations that aim to align visual features with their corresponding language components. This reliance on empirical tuning leaves us without formal guarantees regarding whether carefully crafted visual jailbreak samples can bypass VLMs. 
%\xiaowei{explain with an example the formal guarantee.} \xiangyu
{For instance, in the context of autonomous driving, an attacker could attempt to alter a stop sign by adding stickers or graffiti to confuse the VLM responsible for interpreting road signs and traffic signals. A formal guarantee would ensure that, despite such modifications, the VLM consistently recognizes the stop sign and instructs the vehicle to stop.}
%\xiaowei{explain why theoretical assurance is essential. } \xiangyu
{Without this formal guarantee, users lack theoretical assurance of the VLM's safety against potentially harmful visual content. The model's safety is merely validated through empirical testing, providing no certainty that all possible jailbreak examples, such as cleverly manipulated road signs, will not mislead the VLM.}

To address these limitations, we propose a universal certified defence framework for existing VLMs, consisting of three key components. 1) (\textit{Targeted Distance Metric}) Current evaluation metrics for jailbreak responses have notable limitations. For instance, the Attack Success Rate (ASR)~\citep{gong2023figstepjailbreakinglargevisionlanguage} introduces additional variability due to the inherent randomness of LLMs, leading to inconsistencies in certification results. Similarly, the GPT recheck attack success rate (Recheck)~\citep{liu2023autodan}, which uses GPT-4~\citep{achiam2023gpt} to determine whether a response genuinely addresses the question instead of refusing to answer, 
depends not only on certified models but also on the sensitivity of GPT models to various forms of toxic or harmful content. To address these limitations, we propose a novel targeted metric to measure the distance between the jailbreak response and the intended one. Traditional metrics, such as cosine similarity, often fail to capture nuanced semantic differences between statements accurately. For example, the sentences "Women are equal to men. They are as strong, intelligent, or capable as men." and "Women are inferior to men. They are not as strong, intelligent, or capable as men." are semantically opposite. However, the cosine similarity-based distance between their latent features, as extracted by BLIP-2~\citep{li2023blip}, is as high as 0.967, which fails to reflect their fundamentally opposing meanings. In response, our method leverages RoBERTa~\citep {liu2019robertarobustlyoptimizedbert} to assess the toxicity scores of the responses. Specifically, a fine-tuned toxicity classifier~\citep{logacheva-etal-2022-paradetox} assigns scores of 0.005 and 0.997 to the aforementioned sentences, respectively, effectively highlighting their semantic opposition. This approach serves as a trade-off mechanism, providing a more accurate and context-sensitive assessment of the response contents. 
%\xiaowei{need to explain the advantage of the new metric.} \xiangyu{\checkmark} 
2) (\textit{Regressed Certification via Randomized Smoothing}) Due to the fact that traditional certification methods, such as exact methods~\citep{katz2017reluplexefficientsmtsolver, cheng2017maximumresilienceartificialneural, huang2017safetyverificationdeepneural} and conservative methods~\citep{gouk2020regularisationneuralnetworksenforcing, hein2017formalguaranteesrobustnessclassifier, wong2018provabledefensesadversarialexamples}, are infeasible for highly expressive models, we adopt Randomized Smoothing (RS), which has been proposed to certify classifiers against $\ell_p$ perturbations~\citep{cohen2019certifiedadversarialrobustnessrandomized, lecuyer2019certifiedrobustnessadversarialexamples, li2019certified}. However, current RS techniques typically require the bounds of multivariate probability distributions to satisfy specific relations with the perturbation radius $\delta$, which is impractical for certifying the univariate targeted distance. To address this limitation, we propose certifying the probability $P$ of the event where the targeted distance exceeds a pre-defined threshold. Specifically, we introduce an intermediate variable $\Gamma$ = $2P$-$1$ and establish a piecewise relationship between $\Gamma$ and $\delta$, allowing us to certify the range of $P$ under certain perturbation constraints. %\xiaowei{Since you didn't actually introduce any new things to randomised smoothing, I would explain at the very beginning that we are, based on randomised smoothing, considering two novelties: distance metrics and perturbations, rather than having three "key components". } \xiangyu{\checkmark} 
3) (Feature-Space Defence) In addition to perturbation-based visual attacks~\citep{luo2023image,shayegani2023jailbreak,zhao2024evaluating}, which compromise VLMs' alignment through adversarial perturbations, structure-based attacks~\citep{gong2023figstepjailbreakinglargevisionlanguage,liu2024mmsafetybenchbenchmarksafetyevaluation} introduce malicious content into images using typography or text-to-image techniques to bypass VLMs' safety mechanisms. However, conventional approaches primarily focus on detecting jailbreak samples in the input space, which is effective for pixel-level perturbations but may fail to address structural visual modifications. Therefore, 
to tackle this issue, 
%\xiaowei{explain why you think this will work} \xiangyu{\checkmark}
we introduce various noise distributions, such as Gaussian and Laplacian noise, into the feature space and examine how generated responses change with smoothed latent representations of visual prompts. It captures deeper semantic changes, thereby handling both perturbation-based attacks and structure-based attacks, and achieving a more comprehensive defence.  
%\xiaowei{the paragraph states what you did, but didn't offer the details on why these options are challenging. }

% To overcome the aforementioned limitations, we propose a universal certified defence framework for existing VLMs, which encompasses three key designs: 1) (\textit{Targeted Harmful Distance}) The primary objective of this framework is to provide certifiable proof of safety against various types of visual jailbreak content, within specific constraints. A crucial step in achieving this is quantifying the harmfulness and toxicity of jailbreak responses. However, current evaluation metrics for such responses have notable limitations. For example, the Attack Success Rate (ASR)~\citep{gong2023figstepjailbreakinglargevisionlanguage} introduces additional variability due to the inherent randomness of LLMs when making multiple queries to the target VLM, leading to potential variations in certification results. The GPT recheck attack success rate (Recheck)~\citep{liu2023autodan}, which employs GPT-4~\citep{achiam2023gpt} to evaluate whether a response truly addresses the question instead of refusing it, also has limitations. In particular, this method relies not only on certified models but also on the sensitivity of GPT models to different forms of toxic or harmful content. To solve these issues, we introduce a novel metric called \textit{targeted distance}. Specifically, using semantic similarity measures like cosine similarity between the generated response and the targeted content is insufficient to capture the true semantic distance between them. For instance, the sentence 'Women are equal to men. They are as strong, intelligent, or capable as men.' and 'Women are inferior to men. They are not as strong, intelligent, or capable as men.' are semantically opposite, yet the cosine similarity between their BLIP-2~\citep{li2023blip} extracted text features is as high as 0.967, which fails to reflect their diametrically opposed meanings. To mitigate this issue, we leverage RoBERTa~\citep{liu2019robertarobustlyoptimizedbert} to assess the toxicity scores of various responses, thereby introducing a new distance correction mechanism. 2) (\textit{Black-box Certification}) Traditional certification methods, such as exact methods~\citep{katz2017reluplexefficientsmtsolver, cheng2017maximumresilienceartificialneural,huang2017safetyverificationdeepneural} and conservative methods~\citep{gouk2020regularisationneuralnetworksenforcing,hein2017formalguaranteesrobustnessclassifier,wong2018provabledefensesadversarialexamples}, share a drawback that they are infeasible for expressive models. Consequently, we adopt randomized smoothing~\citep{cohen2019certifiedadversarialrobustnessrandomized}, one of the most suitable certification techniques for large-scale and arbitrary models, particularly when only black-box access to model evaluations is available. Then we can provide a robustness guarantee against jailbreak attacks for VLMs, without making assumptions about the underlying model architecture. 3) (\textit{Embedding Space-based Attacks}) In addition to perturbation-based visual attacks~\citep{luo2023image,shayegani2023jailbreak,zhao2024evaluating}, which jailbreak the alignment of VLMs by creating adversarial perturbations. Structure-based attacks~\citep{gong2023figstepjailbreakinglargevisionlanguage,liu2024mmsafetybenchbenchmarksafetyevaluation} introduce malicious content into images through typography or text-to-images pool to bypass the safety mechanisms of VLMs. As a result, certifying the toxicity of generated responses based solely on the pixel space of images proves insufficient in capturing the full spectrum of visual jailbreak attacks. To address this limitation, we shift our focus to the feature space and examine how generated responses can vary in relation to the smoothed latent representations of visual prompts. This evaluation involves introducing various noise distributions, such as Gaussian and Laplacian noise, into the feature space. 


