\begin{abstract}
Vision Language Models (VLMs) have demonstrated exceptional performance across various multimodal tasks, but they also face serious safety concerns due to vulnerabilities from visual jailbreak attacks. Existing defence strategies, such as model fine-tuning, response evaluation, and prompt perturbation, largely rely on heuristic methods and lack robust theoretical guarantees. We propose a universal certified defence framework focusing on a more comprehensive evaluation of VLM's robustness to address these challenges. Our approach introduces a novel harmful distance to more accurately quantify responses' toxicity, overcoming traditional metrics' limitations. Additionally, we adopt a black-box certification methodology using randomized smoothing, which provides robustness guarantees without dependence on specific model architectures. Finally, we emphasize feature space analysis over pixel-level evaluation, allowing us to detect a broader range of adversarial attempts. This framework establishes a stronger theoretical foundation for VLM safety against visual adversarial content.
\end{abstract}
\vspace{-18pt}
\begin{center}
    {\textcolor{red}{\textbf{WARNING: This paper contains offensive model outputs.}}}
\end{center}